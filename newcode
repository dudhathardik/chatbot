from lib2to3.pgen2.driver import Driver
import time
import os
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
import selenium
from selenium.webdriver.common.by import By
# from webdriver_manager.Chrome import ChromeDriverManager 
import json
PATH = "C:\Program Files (x86)\chromedriver.exe"
driver = webdriver.Chrome(PATH)

driver.get('https://item.jd.com/1634233929.html')
# driver.manage().window().fullscreen();
# from lxml import etree

# def book_scraper():

#     driver = webdriver.Chrome(ChromeDriverManager().install())
#     driver.get('https://bookstash.io/all-books')
#     time.sleep(5)
#     element = driver.find_elements_by_xpath('//*[@id="__next"]/div[1]/div/div/div[2]/div')

#     data = []
#     for cnt, i in enumerate(element):
#         img_url = i.find_elements_by_xpath('//img[@class="sc-eCstlR lgKTxM"]')[cnt].get_attribute('src')
#         url = i.find_elements_by_xpath('//a[@class="sc-bqyKOL hAQzgO"]')[cnt].get_attribute('href')
#         tmp = i.text.split('\n')
#         tmp = {
#             "link": url,
#             "Book Name": tmp[0],
#             "Image_url": img_url,
#             "Author": tmp[1][3:],
#             "Key Ideass": tmp[2] + ' Key Ideas',
#             "Hashtags": tmp[5:],
#         }
#         data.append(tmp)

#     json_object = json.dumps(data,ensure_ascii=False)
#     with open("main_page_data.json", "w", encoding='UTF-8') as outfile:
#         outfile.write(json_object) 
    
# # book_scraper()


# def get_links():

#     f = open ('D:/akshit/books/main_page_data.json', "r")
#     for_links = json.loads(f.read())
#     links= []
#     for i in range(45):
#         links.append(for_links[i]['link'])
#     json_object = json.dumps(links,ensure_ascii=False)
#     with open("link.json", "w", encoding='UTF-8') as outfile:
#         outfile.write(json_object) 
        
# # get_links()

# def get_data():

#     f = open (r'main_page_data copy.json', "r")
#     for_links = json.loads(f.read())

#     for i in range(len(for_links)):

#         driver = webdriver.Chrome(ChromeDriverManager().install())
#         driver.get(for_links[i]["link"])    
#         element = driver.find_element_by_xpath('//button[@class="sc-fubCzh hNzLlm"]').click()
#         time.sleep(5)

#         key_ideas = int(for_links[i]["Key Ideass"].split(" ")[0])
#         ele = []

#         soup = BeautifulSoup(driver.page_source, features="lxml")
#         xml = etree.HTML(str(soup))
#         xml = xml.xpath('//*[@id="__next"]/div[1]/div/div/div[3]/div/div/div/div/div/div/div/div[1]/div[2]') # will give all 8 cards
        
#         for card in xml:
#             ele.append(card.text)
#             print(card.text)

#         exit(0)

#         # element = driver.find_elements_by_xpath('//div[@class="sc-jrAFXE fqNQIG FlipCard_flip_card_back__dE1Fq"]')[0]
#         # ele.append(element.text)
#         # #-----------------------------------------------------------
#         # element = driver.find_element_by_xpath('//div[@class="sc-jrAFXE fqNQIG FlipCard_flip_card_back__dE1Fq"]')
#         # ele.append(element.text)

#         # for k in range(1, key_ideas):
#         #     element = driver.find_element_by_xpath('//div[@class="sc-jrAFXE cHkvum"]').click()
#         #     time.sleep(2)
#         #     # element = driver.find_elements_by_xpath('//div[@class="sc-jrAFXE fqNQIG FlipCard_flip_card_back__dE1Fq"]')
#         #     element = driver.find_elements_by_xpath('//*[@id="__next"]/div[1]/div/div/div[3]/div/div/div/div[1]/div/div/div/div[1]/div[2]')
#         #     ele.append(element.text)
#         # time.sleep(2)
#         # # -------------------------------------------------------------------
#         # element = driver.find_elements_by_xpath('//div[@class="sc-jrAFXE fqNQIG FlipCard_flip_card_back__dE1Fq"]')[-1]
#         # ele.append(element.text)

#         for j in ele:

#             # for k in range(5):
#             #     element = driver.find_element_by_xpath('//button[@class="carousel-arrow block carousel-next"]').click()
#             #     time.sleep(2)
            
#             # element = driver.find_elements_by_xpath('//div[@class="sc-jrAFXE fqNQIG FlipCard_flip_card_back__dE1Fq"]')

#             print(j)
#             tmp = j.split('\n')
#             tmp = {
#                 "Head" : tmp[0],
#                 "Content" : tmp[1:]
#             }
#             print(tmp)

#     driver.close()
#     return tmp


# get_data()

def jd():
    # driver = webdriver.Chrome(ChromeDriverManager().install())
    
    # urls = ['https://item.jd.com/1634233929.html', 'https://item.jd.com/1634233929.html', 'https://item.jd.com/1634233929.html']
    urls = 'https://item.jd.com/1634233929.html'
    # driver.execute_script("window.scrollTo(0, 100000)", '') 
    # selenium.getEval("scrollBy(0, 250)")
     
    # for i in urls:
    driver.get(urls)
    time.sleep(50)
        # time.sleep(3)
    perc = driver.find_element(By.XPATH, '//div[@class="percent-con"]').text
    print(perc)

jd()

driver.quit()